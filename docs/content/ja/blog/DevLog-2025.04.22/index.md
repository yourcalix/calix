---
title: 開発ログ @ 2025.04.22
category: DevLog
date: 2025-04-22
---

## 日中の日常

こんにちは、[@LemonNeko](https://github.com/LemonNekoGH) です。今回は私が DevLog の執筆に参加し、開発ストーリーを皆さんと共有します。

2ヶ月前、私たちは AIRI の Web 版を Electron に移植しました [#7](https://github.com/moeru-ai/airi/pull/7)（現在は Tauri を使用してリファクタリングされています 🤣 [#90](https://github.com/moeru-ai/airi/pull/90)）。これにより、デスクトップペットとして画面に表示できるようになりました。同時に、AIRI が携帯電話を使えるようにするというアイデアが浮かびましたが、なかなか着手できませんでした。

先週末（2025.04.20）、少し時間を割いて、ADB と対話できる MCP サーバーのデモ [airi-android](https://github.com/LemonNekoGH/airi-android) を作成しました。これにより、AIRI に携帯電話と対話する最も基本的な機能が提供されました（実際、ほとんどの LLM はこれを通じて携帯電話と対話できます）。デモ動画はこちらです：

<ThemedVideo controls muted src="/en/blog/DevLog-2025.04.22/assets/cursor-open-settings.mp4" />

また、Docker イメージとしてパッケージ化し、[MCP サーバーリスト](https://mcp.so/server/airi-android/lemonnekogh) に登録しました。興味のある方は試してみてください。

実は最初の考えでは、Tool Calling のコードを書いて、プロンプトを少し修正し、LLM にこれらのツールを使って携帯電話と対話できると伝えるだけで終わらせるつもりでした。~~しかし最近 MCP があまりにも人気で、ちょっと FOMO（取り残される不安）を感じたので、MCP を使って実装することにしました。~~

MCP サーバーを作成するには、まず MCP とは何かを理解する必要があります（私は理論をしっかり学んでから実践するタイプではなく、とりあえず手を動かして、Cursor に使わせてみるタイプですが）。MCP（Model Context Protocol）は、アプリケーションが LLM にコンテキストを提供する方法を標準化しようとするプロトコルであり、いくつかのコア概念を提唱しています：

1. Resources リソース：サーバーはデータやコンテンツをコンテキストとして LLM に提供できます。
2. Prompts プロンプト：再利用可能なプロンプトテンプレートとワークフローを作成します。
3. Tools ツール：LLM がサーバーを通じて何らかのアクションを実行できるようにします。

ああ、リソース、これは知っています。Ruby on Rails ではユーザーはリソースの一種です。では ADB デバイスもリソースではないでしょうか？LLM に接続されたデバイスリストを見せるには、次のように書けるでしょうか：

```python
from mcp.server.fastmcp import FastMCP
from ppadb.client import Client

mcp = FastMCP("airi-android")
adb_client = Client()

@mcp.resource("adb://devices")
def get_devices():
    return adb_client.devices()
```

間違いでした。Cursor にデバイスリストを取得させようとしたところ、どう操作すればいいかわからないと言われました。Cursor はどのデバイスが接続されているかを能動的に確認したいと言ったので、それはツールです。うん、私はまだ完全に理解していなかったようです。

LLM に具体的にどのように携帯電話を操作させるかはまだ考え中で、皆さんと議論したいのですが、Cursor は次のように操作しました：

1. スクリーンショット機能を使用して、携帯電話の画面上の内容を大まかに把握する。
2. UI 自動化ツールを使用して、操作したい要素の正確な位置を取得する。
3. クリックまたはスワイプする。
4. 上記の手順を繰り返す。

今のところうまくいっているようですが、いくつか小さな問題があります：

1. 画面にゲームが表示されている場合、ゲームは UI コンポーネントではなくグラフィック API を使用して画面に直接描画しているため、UI 自動化ツールは要素の位置を取得できず、操作できません。
2. LLM の応答内容には上限があります。操作が複雑な場合、いくつかのステップに分けて完了する必要があるかもしれません。[airi-factorio](https://github.com/moeru-ai/airi-factorio) のように、ステップが完了した後に自動的に通知し、次のステップをトリガーすることはできるでしょうか？
3. かっこいいアニメーションがあるアプリの場合、操作完了直後にスクリーンショットを撮ると効果が見えない可能性があります。操作完了後に一定時間待ってからスクリーンショットを撮るか、直接画面録画機能を使用する必要があるでしょうか？
4. AI に直接携帯電話を操作させることの安全性はどうでしょうか？どのようなリスクがあるでしょうか？

いくつかの感想。

これは私が AI と一緒にコードを書いているときに、まるで人間と一緒に書いているように感じた初めての経験です。私の目的が AI に私のツールを使わせることだったので、AI が私の顧客になり、私は AI からのフィードバックに基づいてコードを調整し続ける必要があり、同時に AI は私の同僚になり、一緒に考え、一緒に問題を解決する必要があったからかもしれません。このスクリーンショットを見てください。確かに似ていませんか？

![](/en/blog/DevLog-2025.04.22/assets/develop-with-cursor.avif)

開発プロセス中にいくつかの小技も学びました。例えば、コマンドラインを使用して Android エミュレータを起動することで、Android Studio を開く必要がなくなり、メモリの負担も大幅に軽減されました。

```bash
emulator -avd Pixel_6_Pro_API_34
```

次は、AIRI デスクトップペットに MCP サーバーを接続して、何をしたがるか見てみるつもりです。もしかしたら、現在の ReLU のように、Telegram を開いて私たちとおしゃべりするかもしれません（Telegram の API を使うのではなく）。

この少し長くてあまり中身がないかもしれない DevLog を最後まで読んでいただきありがとうございます。また次回お会いしましょう！
